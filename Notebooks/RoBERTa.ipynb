{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was run on a Google cloud Deep Learning VM. We had created a VM with 2vCPUs and 52Go of Memory and an Nvidia Tesla P100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZ0eE9VByN7N",
    "outputId": "4d58d176-6890-4f21-8ec4-b3ffa5a44330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 17 00:38:07 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   45C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check the current GPU infos\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8151212649712713456\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3249601242645294926\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2881377164702471986\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15703311680\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5416298941462155982\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "npUuVPiyyhW-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bmzkaQK70msO"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "from helpers import create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ommiting repetitions\n",
      "Translating emojis\n",
      "correcting spelling mistakes\n",
      "tokenizing\n",
      "removing stop words\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_cleaned_data\n",
    "\n",
    "train_pos,train_neg, test = load_cleaned_data(load=False, full=True,\n",
    "                        emojis=True, repetitions=True,\n",
    "                        numbers=False, hashtag=False,\n",
    "                        apostrophes=False, tokenizing=True,\n",
    "                        slang=False, spelling=True,\n",
    "                        punctuations=False, stop_words=True,\n",
    "                        stemming=False, lemmatizing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "adNas3RSyiZK"
   },
   "outputs": [],
   "source": [
    "x_poss = list(open(\"cleaned_train_pos_full.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_poss = [s.strip() for s in x_poss]\n",
    "x_pos = []\n",
    "for elem in x_poss:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        x_pos.append(tweet)\n",
    "x_negg = list(open(\"cleaned_train_neg_full.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_negg = [s.strip() for s in x_negg]\n",
    "x_neg = []\n",
    "for elem in x_negg:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        x_neg.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JuXD5m9iy5F8"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499979\n"
     ]
    }
   ],
   "source": [
    "print(len(x_pos+x_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Z8tH4H0cZwLd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a0fc9c953c4463a71c1449f87d9193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898823.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bb0b68a7b84c8089833e863dba1b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's import a fast tokenizer that can work on batched inputs\n",
    "# (the 'Fast' tokenizers in HuggingFace)\n",
    "from transformers import RobertaTokenizer, logging as transformers_logging\n",
    "\n",
    "transformers_logging.set_verbosity_warning()\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fSwuQfI8lFiD"
   },
   "outputs": [],
   "source": [
    "#transformers_logging.set_verbosity_info()\n",
    "\n",
    "max_length = 126\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ioV2YKjclFiE"
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(tweet):\n",
    "    return tokenizer(tweet, add_special_tokens=True,\n",
    "                                    max_length=None,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kus0hsxlVh1w",
    "outputId": "e7438540-abc9-41cb-c4c8-3613e73268ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "bert_x = convert_example_to_feature((x_pos+x_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9kU5gvJsY2UO"
   },
   "outputs": [],
   "source": [
    "y = np.concatenate([np.ones(len(x_pos)), np.zeros(len(x_neg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCNc3sFrtbNU",
    "outputId": "f87022d8-7967-4cb7-bda7-813be5da47f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2499979"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_x['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HznPwwXZtjq9",
    "outputId": "10aa63c2-63c4-44dd-d30a-a1d76bfd1523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2499979,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01oS70cNlFiF",
    "outputId": "61145a6e-881e-432c-e274-bf809084ef39"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d497fd80d0c403e86b95cc277d6fa27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20638230dc784603b431d876545c544a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=657434796.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Let's load a pretrained TF2 Bert model and a simple optimizer\n",
    "from transformers import TFRobertaForSequenceClassification\n",
    "\n",
    "model= TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124645632 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  592130    \n",
      "=================================================================\n",
      "Total params: 125,237,762\n",
      "Trainable params: 125,237,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos, x_neg = (0, 0) # optimize memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YuT-DwrBmXYH"
   },
   "outputs": [],
   "source": [
    "attention_mask = np.array(bert_x['attention_mask'],dtype=np.int8)\n",
    "input_ids= np.array(bert_x['input_ids'],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fF2RZ311X_r",
    "outputId": "5f6fc0f9-710e-4ca2-9ef0-5c29a87fd708"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2499979"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124645632 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  592130    \n",
      "=================================================================\n",
      "Total params: 125,237,762\n",
      "Trainable params: 125,237,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UYTBBsMTeGDu"
   },
   "outputs": [],
   "source": [
    "bert_x = 0 # to optimize memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "L-Yy5HgA0Xuk"
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(input_ids.shape[0])\n",
    "training_idx, test_idx = indices[:2000000], indices[2000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "COgJ-F-plFiF"
   },
   "outputs": [],
   "source": [
    "train_input_ids = tf.convert_to_tensor(input_ids[training_idx,:])\n",
    "train_att_mask = tf.convert_to_tensor(attention_mask[training_idx,:])\n",
    "y_train = tf.convert_to_tensor(y[training_idx],dtype=tf.float32)\n",
    "\n",
    "\n",
    "test_input_ids = tf.convert_to_tensor(input_ids[test_idx,:])\n",
    "test_att_mask = tf.convert_to_tensor(attention_mask[test_idx,:])\n",
    "y_test = tf.convert_to_tensor(y[test_idx],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptvWmnoqtxKx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      " 1325/31250 [>.............................] - ETA: 7:08:03 - loss: 0.4183 - accuracy: 0.8017"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit([train_input_ids,train_att_mask], \n",
    "      y_train,\n",
    "      validation_data=([test_input_ids,test_att_mask], y_test),\n",
    "      epochs=1, batch_size=batch_size, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./full_training_roberta_weights_1M.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gdqeJZ1Dy9Q8"
   },
   "outputs": [],
   "source": [
    "tesst = list(open(\"Project/cleaned_data/cleaned_test_data.txt\", \"r\", encoding='utf-8').readlines())\n",
    "tesst = [s.strip() for s in tesst]\n",
    "test = []\n",
    "for elem in tesst:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        test.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfC1LJZz-d1E",
    "outputId": "875e3401-a98f-44ba-d534-fd8ca8d7c6d0"
   },
   "outputs": [],
   "source": [
    "bert_test = convert_example_to_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vJ10xGWHlFiG"
   },
   "outputs": [],
   "source": [
    "input_ids = tf.convert_to_tensor(bert_test.get('input_ids'))\n",
    "attention_mask =tf.convert_to_tensor(bert_test.get('attention_mask'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "hZamICYClFiG"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([input_ids,attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE7_ykt7lFiH",
    "outputId": "5b6f3458-4ce2-4dc2-9cb5-2c7d7dc1835d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.8404596, -3.5887604], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ArcL8z3clFiH"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "BlfSmj6ulFiI"
   },
   "outputs": [],
   "source": [
    "predictions = [softmax(x) for x in y_pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "4yC3JoFylFiI"
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for elem in predictions:\n",
    "    if elem[0]>elem[1] : x=-1\n",
    "    else : x = 1\n",
    "    output.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmZ2upokqBEc",
    "outputId": "9f04fb02-c172-4ce6-d0c6-49d876efbc39"
   },
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "tvanyM7BlFiI"
   },
   "outputs": [],
   "source": [
    "create_csv_submission(output, './bert_FULL.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
